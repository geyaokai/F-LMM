{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# å­¦ä¹  Qwen2.5-VL æ¨¡å‹\n",
        "\n",
        "> ğŸ“š **æœ¬ Notebook å¸®åŠ©ä½ ç†è§£å’Œä½¿ç”¨ Qwen2.5-VL æ¨¡å‹**  \n",
        "> åŒ…å«æ¨¡å‹åŠ è½½ã€å›¾åƒå¤„ç†ã€æ¨ç†å’Œä¸ F-LMM çš„é›†æˆ\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“– ç›®å½•\n",
        "\n",
        "1. [Qwen2.5-VL ç®€ä»‹](## 1. Qwen2.5-VL ç®€ä»‹)\n",
        "2. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)\n",
        "3. [æ¨¡å‹åŠ è½½](#æ¨¡å‹åŠ è½½)\n",
        "4. [å›¾åƒå¤„ç†](#å›¾åƒå¤„ç†)\n",
        "5. [åŸºç¡€æ¨ç†](#åŸºç¡€æ¨ç†)\n",
        "6. [éªŒè¯ï¼šå›¾åƒ Token çš„æ‰©å±•æœºåˆ¶](#éªŒè¯å›¾åƒ-token-çš„æ‰©å±•æœºåˆ¶) â­ **é‡è¦**\n",
        "7. [ç†è§£ Token ç¼–ç ](#ç†è§£-token-ç¼–ç )\n",
        "8. [æå–æ³¨æ„åŠ›å’Œéšè—å±‚](#æå–æ³¨æ„åŠ›å’Œéšè—å±‚)\n",
        "9. [ä¸ F-LMM é›†æˆ](#ä¸-f-lmm-é›†æˆ)\n",
        "10. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)\n",
        "11. [å‚è€ƒèµ„æº](#å‚è€ƒèµ„æº)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Qwen2.5-VL ç®€ä»‹\n",
        "\n",
        "### ğŸ¯ æ ¸å¿ƒç‰¹ç‚¹\n",
        "\n",
        "- **åŠ¨æ€åˆ†è¾¨ç‡**ï¼šè‡ªåŠ¨é€‚åº”ä¸åŒå›¾åƒå°ºå¯¸\n",
        "- **åŸç”Ÿ ViT**ï¼špatch_size=14\n",
        "- **å¤šæ¨¡æ€èåˆ**ï¼šå›¾åƒå’Œæ–‡æœ¬æ·±åº¦èåˆ\n",
        "- **ç‰¹æ®Š token**ï¼š`<|vision_start|>` (151652), `<|vision_end|>` (151653), `<|image_pad|>` (151655)\n",
        "\n",
        "### ğŸ“Š ä¸å…¶ä»–æ¨¡å‹å¯¹æ¯”\n",
        "\n",
        "| ç‰¹æ€§ | Qwen2.5-VL | LLaVA | DeepSeek-VL |\n",
        "|------|-----------|-------|-------------|\n",
        "| åˆ†è¾¨ç‡ | åŠ¨æ€ | å›ºå®š | å›ºå®š |\n",
        "| pixel_values | 2D | 4D | 4D |\n",
        "| image_grid_thw | å¿…éœ€ | ä¸éœ€è¦ | ä¸éœ€è¦ |\n",
        "\n",
        "### ğŸ”‘ å…³é”®æ¦‚å¿µ\n",
        "\n",
        "**image_grid_thw**: `[temporal, height_grids, width_grids]`\n",
        "\n",
        "| ç»´åº¦ | å«ä¹‰ | è¯´æ˜ |\n",
        "|------|------|------|\n",
        "| **T (temporal)** | æ—¶é—´ç»´åº¦/å¸§æ•° | å¯¹äº**å•å¼ é™æ€å›¾åƒï¼ŒT=1**ï¼›å¯¹äºè§†é¢‘ï¼ŒT=è§†é¢‘å¸§æ•° |\n",
        "| **H (height)** | é«˜åº¦ç½‘æ ¼æ•° | å›¾åƒåœ¨é«˜åº¦æ–¹å‘ä¸Šè¢«åˆ‡åˆ†æˆå¤šå°‘ä¸ª patch |\n",
        "| **W (width)** | å®½åº¦ç½‘æ ¼æ•° | å›¾åƒåœ¨å®½åº¦æ–¹å‘ä¸Šè¢«åˆ‡åˆ†æˆå¤šå°‘ä¸ª patch |\n",
        "\n",
        "**é‡è¦**: \n",
        "- Qwen2.5-VL å¯¹å•å¼ å›¾åƒ T=1ï¼ˆ**ä¸æ˜¯ 2**ï¼‰\n",
        "- æ—§ç‰ˆ Qwen-VL å¯èƒ½ä½¿ç”¨ T=2ï¼ˆå¤åˆ¶å¸§ï¼‰ï¼Œä½† Qwen2.5-VL å·²æ”¹è¿›\n",
        "- å›¾åƒ token æ€»æ•° = T Ã— H Ã— W (å¯¹äºå•å¼ å›¾åƒå°±æ˜¯ H Ã— W)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ç¯å¢ƒå‡†å¤‡\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.10.19 (main, Oct 21 2025, 16:43:05) [GCC 11.2.0]\n",
            "PyTorch: 2.2.2+cu118\n",
            "CUDA available: True\n",
            "GPU æ•°é‡: 8\n",
            "å½“å‰ GPU: 0\n",
            "GPU åç§°: NVIDIA A800 80GB PCIe\n",
            "Transformers: 4.51.3\n"
          ]
        }
      ],
      "source": [
        "# è®¾ç½®ä½¿ç”¨ 3 å· GPU\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
        "\n",
        "# ä½ çš„ä»£ç†åœ°å€\n",
        "proxy_url = \"http://10.156.216.30:16371\" \n",
        "\n",
        "# è®¾ç½® HTTP å’Œ HTTPS ä»£ç†\n",
        "os.environ['http_proxy'] = proxy_url\n",
        "os.environ['https_proxy'] = proxy_url\n",
        "os.environ['HTTP_PROXY'] = proxy_url # å»ºè®®åŒæ—¶è®¾ç½®å¤§å°å†™\n",
        "os.environ['HTTPS_PROXY'] = proxy_url\n",
        "# æ£€æŸ¥ç¯å¢ƒ\n",
        "import sys\n",
        "print(f\"Python: {sys.version}\")\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"PyTorch: {torch.__version__}\")\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU æ•°é‡: {torch.cuda.device_count()}\")\n",
        "        print(f\"å½“å‰ GPU: {torch.cuda.current_device()}\")\n",
        "        print(f\"GPU åç§°: {torch.cuda.get_device_name(0)}\")\n",
        "except ImportError:\n",
        "    print(\"PyTorch not installed!\")\n",
        "\n",
        "try:\n",
        "    import transformers\n",
        "    print(f\"Transformers: {transformers.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"Transformers not installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… å¯¼å…¥æˆåŠŸï¼\n"
          ]
        }
      ],
      "source": [
        "# å¯¼å…¥å¿…è¦çš„åº“\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"âœ… å¯¼å…¥æˆåŠŸï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. æ¨¡å‹åŠ è½½\n",
        "\n",
        "### 3.1 åŠ è½½ Processor å’Œæ¨¡å‹\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "åŠ è½½ Processor: Qwen/Qwen2.5-VL-3B-Instruct\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processor åŠ è½½æˆåŠŸ\n",
            "   ç±»å‹: <class 'transformers.models.qwen2_5_vl.processing_qwen2_5_vl.Qwen2_5_VLProcessor'>\n"
          ]
        }
      ],
      "source": [
        "# æ¨¡å‹åç§°\n",
        "model_name = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "print(f\"åŠ è½½ Processor: {model_name}\")\n",
        "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "print(f\"âœ… Processor åŠ è½½æˆåŠŸ\")\n",
        "print(f\"   ç±»å‹: {type(processor)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "merge_size: None\n"
          ]
        }
      ],
      "source": [
        "merge_size=getattr(processor, 'merge_size', None)\n",
        "print(f\"merge_size: {merge_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Vision Token IDs:\n",
            "  <image>: None\n",
            "  <|vision_start|>: 151652\n",
            "  <|vision_end|>: 151653\n",
            "  <|image_pad|>: 151655\n"
          ]
        }
      ],
      "source": [
        "# æ£€æŸ¥ç‰¹æ®Š token\n",
        "tokenizer = processor.tokenizer\n",
        "vision_tokens = {\n",
        "    '<image>': tokenizer.convert_tokens_to_ids('<|image|>'),\n",
        "    '<|vision_start|>': tokenizer.convert_tokens_to_ids('<|vision_start|>'),\n",
        "    '<|vision_end|>': tokenizer.convert_tokens_to_ids('<|vision_end|>'),\n",
        "    '<|image_pad|>': tokenizer.convert_tokens_to_ids('<|image_pad|>')\n",
        "}\n",
        "\n",
        "print(\"\\nVision Token IDs:\")\n",
        "for token, token_id in vision_tokens.items():\n",
        "    print(f\"  {token}: {token_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ–¹å¼1ï¼šå®˜æ–¹æ¨èï¼ˆç”¨ dictï¼‰\n",
        "messages = [{\n",
        "    \"role\": \"user\", \n",
        "    \"content\": [\n",
        "        {\"type\": \"image\"},  # ä¸ç›´æ¥å†™ <image>\n",
        "        {\"type\": \"text\", \"text\": \"test\"}\n",
        "    ]\n",
        "}]\n",
        "text1 = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "print(\"æ–¹å¼1è¾“å‡ºï¼š\", text1)\n",
        "\n",
        "# æ–¹å¼2ï¼šç›´æ¥åœ¨æ–‡æœ¬é‡Œå†™ <image>\n",
        "text2 = \"<image>test\"\n",
        "print(\"æ–¹å¼2è¾“å‡ºï¼š\", text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡åŠ è½½ä¼šä¸‹è½½ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼‰\n",
        "print(\"åŠ è½½æ¨¡å‹...\")\n",
        "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation=\"eager\",\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼\")\n",
        "print(f\"   è®¾å¤‡: {device}\")\n",
        "print(f\"   å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "configuration = model.config\n",
        "configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "print(result.stdout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. å›¾åƒå¤„ç†å’Œæ¨ç†\n",
        "\n",
        "### 4.1 å‡†å¤‡æµ‹è¯•å›¾åƒ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ›å»ºæµ‹è¯•å›¾åƒï¼ˆæˆ–åŠ è½½çœŸå®å›¾åƒï¼‰\n",
        "def create_test_image(size=(640, 480)):\n",
        "    return Image.new('RGB', size, color=(100, 150, 200))\n",
        "\n",
        "# æˆ–ä½¿ç”¨: image = Image.open('path/to/image.jpg')\n",
        "image = Image.open('data/cot/flickr30k/807129.jpg')\n",
        "# image = create_test_image()\n",
        "\n",
        "print(f\"å›¾åƒå°ºå¯¸: {image.size}\")\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.imshow(image)\n",
        "plt.title(f\"æµ‹è¯•å›¾åƒ ({image.size[0]}x{image.size[1]})\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 å¤„ç†å›¾åƒå¹¶æŸ¥çœ‹ inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ„å»ºæ¶ˆæ¯\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": image},\n",
        "            {\"type\": \"text\", \"text\": \"<image>Describe this image.\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# å¤„ç† tokennizeä¸ºTrueçš„è¯è¾“å‡ºçš„æ˜¯token\n",
        "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "print(text)\n",
        "inputs = processor(text=[text], images=[image], return_tensors=\"pt\", padding=True)\n",
        "inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
        "\n",
        "print(\"å¤„ç†åçš„è¾“å…¥:\")\n",
        "for key, value in inputs.items():\n",
        "    if isinstance(value, torch.Tensor):\n",
        "        print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "# åˆ†æ image_grid_thw\n",
        "image_grid_thw = inputs['image_grid_thw']\n",
        "t, h, w = image_grid_thw[0].tolist()\n",
        "print(f\"\\nğŸ“Š image_grid_thw: [t={t}, h={h}, w={w}]\")\n",
        "print(f\"   å›¾åƒ token æ•°é‡: {h * w}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç›´æ¥ä½¿ç”¨image_processorå¤„ç†å›¾åƒ\n",
        "image_inputs = processor.image_processor(images=image, return_tensors=\"pt\")\n",
        "print(\"Image processor output:\")\n",
        "print(f\"pixel_values shape: {image_inputs['pixel_values'].shape}\")\n",
        "print(f\"image_grid_thw: {image_inputs['image_grid_thw']}\")\n",
        "\n",
        "# è®¡ç®—patchæ•°é‡\n",
        "patch_size = 14\n",
        "height, width = image.size\n",
        "scaled_h = (height + patch_size - 1) // patch_size\n",
        "scaled_w = (width + patch_size - 1) // patch_size\n",
        "print(f\"\\nManual calculation:\")\n",
        "print(f\"Original image size: {width}x{height}\")\n",
        "print(f\"Scaled to patches: {scaled_h}x{scaled_w} patches\")\n",
        "print(f\"Total patches: {scaled_h * scaled_w}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " `image_grid_thw` æ˜¯åœ¨ **Qwen-VL**ï¼ˆé€šä¹‰åƒé—®è§†è§‰è¯­è¨€æ¨¡å‹ï¼‰ç³»åˆ—æ¨¡å‹ä¸­ï¼Œç”¨äºå¤„ç†å¤šæ¨¡æ€è¾“å…¥æ—¶çš„ä¸€ä¸ª**å…³é”®å‚æ•°**ã€‚\n",
        "\n",
        "è¿™é‡Œçš„ `t` ä»£è¡¨çš„æ˜¯ï¼š\n",
        "\n",
        "***\n",
        "\n",
        "### T: Temporal (æ—¶é—´ç»´åº¦/å¸§æ•°)\n",
        "\n",
        "åœ¨ `image_grid_thw` è¿™ä¸ªå››å…ƒç»„ä¸­ï¼Œå„ä¸ªå­—æ¯çš„å«ä¹‰å¦‚ä¸‹ï¼š\n",
        "\n",
        "| å­—æ¯ | è‹±æ–‡å«ä¹‰ | ä¸­æ–‡å«ä¹‰ | ä½œç”¨è¯´æ˜ |\n",
        "| :---: | :---: | :---: | :--- |\n",
        "| **T** | **Temporal** | **æ—¶é—´/å¸§æ•°** | **å›¾åƒåºåˆ—çš„é•¿åº¦ã€‚** å¯¹äºå•å¼ é™æ€å›¾åƒï¼ŒQwen-VL ä¸ºäº†ç»Ÿä¸€å¤„ç†æµç¨‹ï¼ˆä½¿å…¶ä¸è§†é¢‘å¤„ç†å…¼å®¹ï¼Œå¹¶åˆ©ç”¨å…¶ 3D å·ç§¯çš„ Patch æå–ï¼‰ |\n",
        "| **H** | **Height** | **é«˜åº¦** | å›¾åƒ/å¸§è¢«åˆ‡åˆ†æˆ Patch åçš„**é«˜åº¦æ–¹å‘çš„ Patch æ•°é‡**ã€‚ |\n",
        "| **W** | **Width** | **å®½åº¦** | å›¾åƒ/å¸§è¢«åˆ‡åˆ†æˆ Patch åçš„**å®½åº¦æ–¹å‘çš„ Patch æ•°é‡**ã€‚ |\n",
        "\n",
        "**ç®€è€Œè¨€ä¹‹ï¼Œ`image_grid_thw` ç”¨äºè®°å½•è§†è§‰ç‰¹å¾åœ¨æ—¶é—´å’Œç©ºé—´ä¸Šçš„ç½‘æ ¼å¤§å°ï¼ŒæŒ‡å¯¼æ¨¡å‹å¦‚ä½•è§£é‡Šå’Œå¤„ç†è¾“å…¥çš„è§†è§‰ Tokensã€‚**\n",
        "\n",
        "å¯¹äº Qwen2.5-VL è¿™ç§å¤šæ¨¡æ€æ¨¡å‹ï¼š\n",
        "\n",
        "* å½“è¾“å…¥æ˜¯**å•å¼ å›¾ç‰‡**æ—¶ï¼Œ`T` çš„å€¼é€šå¸¸è¢«è®¾å®šä¸º **1** \n",
        "* å½“è¾“å…¥æ˜¯**å¤šå¸§è§†é¢‘**æ—¶ï¼Œ`T` å°±ä»£è¡¨è§†é¢‘çš„**å¸§æ•°**ã€‚\n",
        "\n",
        "è¿™ä¸ªå‚æ•°ç¡®ä¿äº†è§†è§‰ç‰¹å¾åœ¨è¿›å…¥æ¨¡å‹çš„ Transformer ç¼–ç å™¨ä¹‹å‰ï¼Œèƒ½å¤Ÿè¢«æ­£ç¡®åœ°è¿›è¡Œå¤šæ¨¡æ€æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆMulti-modal RoPEï¼‰ç­‰æ“ä½œã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. æ¨ç†å’Œæå–æ³¨æ„åŠ›\n",
        "\n",
        "### 5.1 ç”Ÿæˆæ–‡æœ¬\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. éªŒè¯ï¼šå›¾åƒ Token çš„æ‰©å±•æœºåˆ¶\n",
        "\n",
        "### ğŸ”¬ **æ ¸å¿ƒé—®é¢˜**ï¼šQwen2.5-VL å¦‚ä½•å°†å›¾åƒè½¬æ¢ä¸º tokensï¼Ÿ\n",
        "\n",
        "**å…³é”®ç–‘é—®**ï¼š\n",
        "- F-LMM é…ç½®æ–‡ä»¶ä¸­ä½¿ç”¨ `<image>` å ä½ç¬¦\n",
        "- Qwen tokenizer è¯è¡¨ä¸­æœ‰ `<|image_pad|>` token\n",
        "- è¿™ä¸¤è€…æ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿæ‰©å±•è¿‡ç¨‹æ˜¯æ€æ ·çš„ï¼Ÿ\n",
        "\n",
        "è®©æˆ‘ä»¬é€šè¿‡ 4 ä¸ªå®éªŒæ¥æ­ç¤ºå®Œæ•´æµç¨‹ï¼\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== å®éªŒ 1: æ£€æŸ¥ tokenizer è¯è¡¨ä¸­çš„ç‰¹æ®Š tokens ==========\n",
        "print(\"=\" * 80)\n",
        "print(\"ã€å®éªŒ 1ã€‘æ£€æŸ¥ tokenizer è¯è¡¨ä¸­çš„ç‰¹æ®Š tokens\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "tokenizer = processor.tokenizer\n",
        "\n",
        "# æµ‹è¯•å…³é”® tokens\n",
        "test_tokens = [\n",
        "    \"<image>\",           # ç”¨æˆ·ä½¿ç”¨çš„å ä½ç¬¦\n",
        "    \"<|image_pad|>\",    # å®é™…çš„å›¾åƒ padding token\n",
        "    \"<|vision_start|>\",  # Vision åºåˆ—å¼€å§‹\n",
        "    \"<|vision_end|>\",    # Vision åºåˆ—ç»“æŸ\n",
        "]\n",
        "\n",
        "print(f\"\\n{'Token':<20} | {'ID':>10} | {'ç¼–ç å IDs':>20} | è§£ç å\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for token in test_tokens:\n",
        "    try:\n",
        "        # æ–¹æ³•1: convert_tokens_to_ids (å°† token å­—ç¬¦ä¸²è½¬ä¸º ID)\n",
        "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "        \n",
        "        # æ–¹æ³•2: encode (å°†å­—ç¬¦ä¸²ç¼–ç ä¸º ID åºåˆ—)\n",
        "        encoded_ids = tokenizer.encode(token, add_special_tokens=False)\n",
        "        \n",
        "        # è§£ç å›æ–‡æœ¬\n",
        "        decoded = tokenizer.decode(encoded_ids, skip_special_tokens=False)\n",
        "        \n",
        "        print(f\"{token:<20} | {token_id:>10} | {str(encoded_ids):>20} | '{decoded}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"{token:<20} | ERROR: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== å®éªŒ 2: apply_chat_template å¦‚ä½•æ’å…¥å ä½ç¬¦ ==========\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ã€å®éªŒ 2ã€‘apply_chat_template å¦‚ä½•å¤„ç†å›¾åƒ\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ä½¿ç”¨ messages æ ¼å¼\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": image},\n",
        "            {\"type\": \"text\", \"text\": \"Describe this image.\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# ç”Ÿæˆå¸¦å ä½ç¬¦çš„æ–‡æœ¬\n",
        "text_with_placeholder = processor.apply_chat_template(\n",
        "    messages, \n",
        "    tokenize=False,  # å…ˆä¸ tokenizeï¼Œåªç”Ÿæˆæ–‡æœ¬\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "print(f\"\\napply_chat_template ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆtokenize=Falseï¼‰:\")\n",
        "print(\"-\" * 80)\n",
        "print(text_with_placeholder)\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# æ£€æŸ¥åŒ…å«å“ªäº›å ä½ç¬¦\n",
        "has_image = \"<image>\" in text_with_placeholder\n",
        "has_vision_start = \"<|vision_start|>\" in text_with_placeholder\n",
        "has_image_pad = \"<|image_pad|>\" in text_with_placeholder\n",
        "has_vision_end = \"<|vision_end|>\" in text_with_placeholder\n",
        "\n",
        "print(f\"\\nã€å‘ç°ã€‘:\")\n",
        "print(f\"  - åŒ…å« '<image>': {has_image}\")\n",
        "print(f\"  - åŒ…å« '<|vision_start|>': {has_vision_start}\")\n",
        "print(f\"  - åŒ…å« '<|image_pad|>': {has_image_pad}\")\n",
        "print(f\"  - åŒ…å« '<|vision_end|>': {has_vision_end}\")\n",
        "\n",
        "# ç»Ÿè®¡ <|image_pad|> æ•°é‡\n",
        "image_pad_count_in_text = text_with_placeholder.count(\"<|image_pad|>\")\n",
        "print(f\"\\n  âœ“ apply_chat_template æ’å…¥äº† {image_pad_count_in_text} ä¸ª '<|image_pad|>' å ä½ç¬¦\")\n",
        "print(f\"  (æ³¨æ„ï¼šè¿™åªæ˜¯åˆå§‹å ä½ç¬¦ï¼Œç¨åä¼šè¢«æ‰©å±•)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== å®éªŒ 3: processor.__call__ å¦‚ä½•æ‰©å±•å ä½ç¬¦ ==========\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ã€å®éªŒ 3ã€‘processor.__call__ å¦‚ä½•å°† 1 ä¸ª <|image_pad|> æ‰©å±•ä¸º N ä¸ª\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"ä»å®éªŒ2æˆ‘ä»¬çŸ¥é“ï¼Œapply_chat_template åªæ’å…¥äº† {image_pad_count_in_text} ä¸ª <|image_pad|>\")\n",
        "print(f\"ç°åœ¨æˆ‘ä»¬çœ‹ processor(text + images) å¦‚ä½•æ ¹æ®å›¾åƒå°ºå¯¸æ‰©å±•å®ƒ...\")\n",
        "\n",
        "# Step 1: å¤„ç†å®Œæ•´çš„æ–‡æœ¬+å›¾åƒ\n",
        "inputs_full = processor(\n",
        "    text=[text_with_placeholder],  # åŒ…å« <image> çš„æ–‡æœ¬\n",
        "    images=[image],                 # å®é™…å›¾åƒ\n",
        "    return_tensors=\"pt\",\n",
        "    padding=False\n",
        ")\n",
        "\n",
        "print(f\"\\nå¤„ç†åçš„ç»“æœ:\")\n",
        "print(f\"  - input_ids shape: {inputs_full['input_ids'].shape}\")\n",
        "print(f\"  - pixel_values shape: {inputs_full['pixel_values'].shape}\")\n",
        "\n",
        "if 'image_grid_thw' in inputs_full:\n",
        "    T, H, W = inputs_full['image_grid_thw'][0].tolist()\n",
        "    print(f\"  - image_grid_thw: T={T}, H={H}, W={W}\")\n",
        "    expected_image_tokens = T * H * W//4\n",
        "    print(f\"  - é¢„æœŸå›¾åƒ tokens æ•°é‡: {T} Ã— {H} Ã— {W} = {expected_image_tokens}\")\n",
        "\n",
        "# Step 2: ç»Ÿè®¡ <|image_pad|> çš„æ•°é‡\n",
        "image_pad_token_id = tokenizer.convert_tokens_to_ids(\"<|image_pad|>\")\n",
        "input_ids_list = inputs_full['input_ids'][0].tolist()\n",
        "image_pad_count = input_ids_list.count(image_pad_token_id)\n",
        "\n",
        "print(f\"\\nã€å…³é”®è¯æ®ã€‘:\")\n",
        "print(f\"  - <|image_pad|> token ID: {image_pad_token_id}\")\n",
        "print(f\"  - input_ids ä¸­ <|image_pad|> çš„æ•°é‡: {image_pad_count}\")\n",
        "print(f\"  - é¢„æœŸæ•°é‡ï¼ˆä» image_grid_thwï¼‰: {expected_image_tokens}\")\n",
        "\n",
        "if image_pad_count == expected_image_tokens:\n",
        "    print(f\"\\n  âœ… å®Œç¾åŒ¹é…ï¼\")\n",
        "    print(f\"  âœ… è¿™è¯æ˜äº†ï¼šprocessor.__call__ å°† {image_pad_count_in_text} ä¸ª <|image_pad|> å ä½ç¬¦\")\n",
        "    print(f\"     æ‰©å±•ä¸ºäº† {image_pad_count} ä¸ª <|image_pad|> tokensï¼\")\n",
        "    print(f\"  âœ… æ‰©å±•å€æ•°: {image_pad_count}/{image_pad_count_in_text} = {image_pad_count//image_pad_count_in_text if image_pad_count_in_text > 0 else 'N/A'}\")\n",
        "else:\n",
        "    print(f\"\\n  âš ï¸  å·®å¼‚: {abs(image_pad_count - expected_image_tokens)}\")\n",
        "\n",
        "# Step 3: æŸ¥çœ‹ç‰¹æ®Š tokens çš„ä½ç½®\n",
        "print(f\"\\nã€Token åˆ†å¸ƒã€‘:\")\n",
        "vision_start_id = tokenizer.convert_tokens_to_ids(\"<|vision_start|>\")\n",
        "vision_end_id = tokenizer.convert_tokens_to_ids(\"<|vision_end|>\")\n",
        "\n",
        "vision_start_pos = [i for i, tid in enumerate(input_ids_list) if tid == vision_start_id]\n",
        "vision_end_pos = [i for i, tid in enumerate(input_ids_list) if tid == vision_end_id]\n",
        "image_pad_first = input_ids_list.index(image_pad_token_id) if image_pad_count > 0 else -1\n",
        "image_pad_last = len(input_ids_list) - 1 - input_ids_list[::-1].index(image_pad_token_id) if image_pad_count > 0 else -1\n",
        "\n",
        "print(f\"  - <|vision_start|> ä½ç½®: {vision_start_pos}\")\n",
        "print(f\"  - <|image_pad|> é¦–æ¬¡å‡ºç°: ä½ç½® {image_pad_first}\")\n",
        "print(f\"  - <|image_pad|> æœ€åå‡ºç°: ä½ç½® {image_pad_last}\")\n",
        "print(f\"  - <|vision_end|> ä½ç½®: {vision_end_pos}\")\n",
        "print(f\"  - æ€» token æ•°: {len(input_ids_list)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== å®éªŒ 4: å¯è§†åŒ–å®Œæ•´çš„ Token åºåˆ— ==========\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ã€å®éªŒ 4ã€‘å¯è§†åŒ–å®Œæ•´çš„ Token åºåˆ—\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# è§£ç æ¯ä¸ª token\n",
        "print(f\"\\nå®Œæ•´ input_ids åºåˆ— (å‰ 50 ä¸ª tokens):\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for i, token_id in enumerate(input_ids_list[:50]):\n",
        "    # è§£ç å•ä¸ª token\n",
        "    token_text = tokenizer.decode([token_id], skip_special_tokens=False)\n",
        "    \n",
        "    # æ ‡è®°ç‰¹æ®Š tokens\n",
        "    if token_id == image_pad_token_id:\n",
        "        marker = \" ğŸ‘‰ <|image_pad|>\"\n",
        "    elif token_id == vision_start_id:\n",
        "        marker = \" ğŸ”µ <|vision_start|>\"\n",
        "    elif token_id == vision_end_id:\n",
        "        marker = \" ğŸ”´ <|vision_end|>\"\n",
        "    else:\n",
        "        marker = \"\"\n",
        "    \n",
        "    print(f\"  [{i:3d}] ID={token_id:6d} â†’ '{token_text}'{marker}\")\n",
        "\n",
        "if len(input_ids_list) > 50:\n",
        "    print(f\"  ... (çœç•¥å‰©ä½™ {len(input_ids_list) - 50} ä¸ª tokens)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ã€ç»“è®ºã€‘\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\"\"\n",
        "é€šè¿‡ä»¥ä¸Š 4 ä¸ªå®éªŒï¼Œæˆ‘ä»¬æ­ç¤ºäº† Qwen2.5-VL å¤„ç†å›¾åƒ token çš„å®Œæ•´æµç¨‹ï¼š\n",
        "\n",
        "1ï¸âƒ£  apply_chat_template(messages) ä¼šæ£€æµ‹åˆ°å›¾åƒï¼Œæ’å…¥åˆå§‹å ä½ç¬¦åºåˆ—ï¼š\n",
        "    <|vision_start|><|image_pad|><|vision_end|>ï¼ˆåªæœ‰ 1 ä¸ª <|image_pad|>ï¼‰\n",
        "\n",
        "2ï¸âƒ£  processor(text, images) ä¼šæ ¹æ®å®é™…å›¾åƒå°ºå¯¸ï¼ˆimage_grid_thwï¼‰ï¼š\n",
        "    - è®¡ç®—éœ€è¦çš„ token æ•°é‡: N = T Ã— H Ã— W//4ï¼ˆmerge_size=2ï¼‰\n",
        "    - å°†é‚£ 1 ä¸ª <|image_pad|> æ‰©å±•ä¸º N ä¸ª <|image_pad|>\n",
        "\n",
        "3ï¸âƒ£  æœ€ç»ˆçš„ input_ids åŒ…å«ï¼š\n",
        "    - æ–‡æœ¬ tokens\n",
        "    - N ä¸ªå›¾åƒ tokensï¼ˆéƒ½æ˜¯ <|image_pad|> çš„ token IDï¼‰\n",
        "    - å…¶ä»–ç‰¹æ®Š tokensï¼ˆ<|vision_start|>, <|vision_end|> ç­‰ï¼‰\n",
        "\n",
        "4ï¸âƒ£  å…³é”®åŒºåˆ«ï¼š\n",
        "    - <image>: è¿™æ˜¯ç”¨æˆ·åœ¨é…ç½®æ–‡ä»¶ä¸­ä½¿ç”¨çš„å­—ç¬¦ä¸²ï¼ˆF-LMM é¡¹ç›®ä¸­ï¼‰\n",
        "    - <|image_pad|>: è¿™æ˜¯ Qwen tokenizer è¯è¡¨ä¸­çœŸæ­£çš„ token\n",
        "    - apply_chat_template ç›´æ¥ä½¿ç”¨ <|vision_*|> æ ¼å¼ï¼Œä¸ä½¿ç”¨ <image>\n",
        "\n",
        "è¿™å°±æ˜¯ Qwen2.5-VL å¤„ç†å›¾åƒ token çš„å®Œæ•´æœºåˆ¶ï¼\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ’¡ è¡¥å……è¯´æ˜ï¼šF-LMM é…ç½®æ–‡ä»¶ä¸­çš„ `<image>` æ˜¯ä»€ä¹ˆï¼Ÿ\n",
        "\n",
        "åœ¨ F-LMM çš„é…ç½®æ–‡ä»¶ä¸­ä½ ä¼šçœ‹åˆ°ï¼š\n",
        "```python\n",
        "prompt = '<image>' + \"Please give me a description of the image.\"\n",
        "image_token = '<image>'\n",
        "```\n",
        "\n",
        "**`<image>` çš„çœŸå®ä½œç”¨**ï¼š\n",
        "1. **å®ƒæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²æ ‡è®°**ï¼Œç”¨äºåœ¨ prompt ä¸­æŒ‡ç¤º\"å›¾åƒåº”è¯¥æ”¾åœ¨è¿™é‡Œ\"\n",
        "2. **ä½†å®ƒä¸ä¼šç›´æ¥å‡ºç°åœ¨ `apply_chat_template` çš„è¾“å‡ºä¸­**ï¼ˆå¦‚å®éªŒ2æ‰€ç¤ºï¼‰\n",
        "3. **F-LMM çš„å¤„ç†æµç¨‹**ï¼š\n",
        "   ```python\n",
        "   # é…ç½®æ–‡ä»¶ä¸­çš„ prompt\n",
        "   prompt = \"<image>Describe this image.\"\n",
        "   \n",
        "   # F-LMM ä¼šæ„é€  messages æ ¼å¼\n",
        "   messages = [{\n",
        "       \"role\": \"user\", \n",
        "       \"content\": [\n",
        "           {\"type\": \"image\", \"image\": actual_image},\n",
        "           {\"type\": \"text\", \"text\": \"Describe this image.\"}\n",
        "       ]\n",
        "   }]\n",
        "   \n",
        "   # apply_chat_template å°†å…¶è½¬æ¢ä¸º\n",
        "   # \"<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\"\n",
        "   ```\n",
        "\n",
        "**æ€»ç»“**ï¼š\n",
        "- `<image>` æ˜¯ F-LMM é¡¹ç›®çš„**çº¦å®šå­—ç¬¦ä¸²**ï¼Œç”¨äºé…ç½®å’Œä»£ç ä¸­æ ‡è®°å›¾åƒä½ç½®\n",
        "- Qwen çš„ `apply_chat_template` ä¸ä½¿ç”¨ `<image>`ï¼Œè€Œæ˜¯ç›´æ¥ç”Ÿæˆ `<|vision_start|><|image_pad|><|vision_end|>` åºåˆ—\n",
        "- F-LMM çš„åŒ…è£…ç±»ï¼ˆ`QwenImageProcessorWrapper`ï¼‰è´Ÿè´£å¤„ç†è¿™ä¸ªè½¬æ¢\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç”Ÿæˆæ–‡æœ¬\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(**inputs, max_new_tokens=50)\n",
        "    \n",
        "# ä»æ¨¡å‹å®Œæ•´çš„ç”Ÿæˆåºåˆ—ä¸­ï¼Œå»æ‰åŸå§‹è¾“å…¥ prompt çš„éƒ¨åˆ†ï¼Œä¿ç•™æ¨¡å‹çœŸæ­£ç”Ÿæˆçš„æ–° tokensã€‚\n",
        "trimmed_ids = [\n",
        "    out_ids[len(in_ids):]\n",
        "    for in_ids, out_ids in zip(inputs['input_ids'], output_ids)\n",
        "]\n",
        "\n",
        "generated_text = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
        "generated_text_trimmed = processor.batch_decode(trimmed_ids, skip_special_tokens=True)[0]\n",
        "print(\"=\"*50)\n",
        "print(\"ç”Ÿæˆçš„æ–‡æœ¬:\")\n",
        "print(\"=\"*50)\n",
        "print(generated_text)\n",
        "print(\"=\"*50)\n",
        "print(\"=\"*50)\n",
        "print(\"ç”Ÿæˆçš„trimmedæ–‡æœ¬:\")\n",
        "print(\"=\"*50)\n",
        "print(generated_text_trimmed)\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 æå–æ³¨æ„åŠ›å’Œéšè—å±‚ï¼ˆF-LMM éœ€è¦ï¼‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æå–æ³¨æ„åŠ›å’Œéšè—å±‚\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs, output_hidden_states=True, output_attentions=True)\n",
        "\n",
        "print(\"âœ… æå–å®Œæˆï¼\")\n",
        "print(f\"   logits: {outputs.logits.shape}\")\n",
        "print(f\"   hidden_states: {len(outputs.hidden_states)} å±‚\")\n",
        "print(f\"   attentions: {len(outputs.attentions)} å±‚\")\n",
        "print(f\"   æ¯å±‚æ³¨æ„åŠ›å½¢çŠ¶: {outputs.attentions[0].shape}\")\n",
        "\n",
        "# æ‰¾åˆ°å›¾åƒ token çš„ä½ç½®\n",
        "input_ids = inputs['input_ids'][0]\n",
        "vision_start_id, vision_end_id = 151652, 151653\n",
        "vision_start_mask = (input_ids == vision_start_id)\n",
        "vision_end_mask = (input_ids == vision_end_id)\n",
        "\n",
        "if vision_start_mask.any() and vision_end_mask.any():\n",
        "    start_pos = vision_start_mask.nonzero()[0].item()\n",
        "    end_pos = vision_end_mask.nonzero()[0].item()\n",
        "    print(f\"\\nğŸ“ å›¾åƒ token ä½ç½®: [{start_pos+1}, {end_pos})\")\n",
        "    print(f\"   æ•°é‡: {end_pos - start_pos - 1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ä¸ F-LMM é›†æˆ\n",
        "\n",
        "### F-LMM ä½¿ç”¨ Qwen çš„å…³é”®æ­¥éª¤ï¼š\n",
        "\n",
        "1. **å†»ç»“ Qwen æ¨¡å‹å‚æ•°** - `model.requires_grad_(False)`\n",
        "2. **æå– hidden_states å’Œ attentions** - å¦‚ä¸Šæ‰€ç¤º\n",
        "3. **æŒ‰ mask_ids åˆ†ç»„æ³¨æ„åŠ›** - æ ¹æ®æ–‡æœ¬ token å¯¹åº”çš„ mask\n",
        "4. **é‡å¡‘æ³¨æ„åŠ›ä¸ºç©ºé—´ç»´åº¦** - ä½¿ç”¨ image_grid_thw\n",
        "5. **é€šè¿‡ UNet ç”Ÿæˆç²—ç²’åº¦ mask**\n",
        "6. **ä½¿ç”¨ SAM ç»†åŒ– mask**\n",
        "\n",
        "### å…³é”®ä»£ç ç¤ºä¾‹ï¼ˆåœ¨ flmm/models/frozen_qwen.py ä¸­ï¼‰ï¼š\n",
        "\n",
        "```python\n",
        "# æå–å›¾åƒ token çš„æ³¨æ„åŠ›\n",
        "images_seq_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
        "images_seq_mask[start_pos+1:end_pos] = True\n",
        "\n",
        "attentions = []\n",
        "for attn in outputs.attentions:\n",
        "    attn_image = attn[:, :, images_seq_mask]\n",
        "    attentions.append(attn_image)\n",
        "\n",
        "# é‡å¡‘ä¸ºç©ºé—´ç»´åº¦\n",
        "attentions = [attn.view(*attn.shape[:-1], h, w) for attn in attentions]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. å¸¸è§é—®é¢˜\n",
        "\n",
        "### Q1: ä¸ºä»€ä¹ˆéœ€è¦ image_grid_thwï¼Ÿ\n",
        "**A:** Qwen2.5-VL ä½¿ç”¨åŠ¨æ€åˆ†è¾¨ç‡ï¼Œéœ€è¦ `image_grid_thw` å‘Šè¯‰æ¨¡å‹å›¾åƒè¢«åˆ†æˆäº†å¤šå°‘ä¸ª patchã€‚ç¼ºå°‘å®ƒä¼šå¯¼è‡´ `TypeError`ã€‚\n",
        "\n",
        "### Q2: pixel_values ä¸ºä»€ä¹ˆæ˜¯ 2D çš„ï¼Ÿ\n",
        "**A:** è¿™æ˜¯ Qwen çš„ç‰¹æ®Šè®¾è®¡ã€‚ä¼ ç»Ÿæ¨¡å‹ç”¨ 4D `[B, C, H, W]`ï¼ŒQwen ç”¨ 2D `[num_patches, patch_dim]`ã€‚\n",
        "\n",
        "### Q3: Token æ•°é‡ä¸åŒ¹é…çš„è­¦å‘Šæ­£å¸¸å—ï¼Ÿ\n",
        "**A:** æ˜¯çš„ï¼`Warning: Image token count X != Y` æ˜¯æ­£å¸¸çš„ï¼Œä»£ç ä¼šè‡ªåŠ¨æ¨æ–­æ­£ç¡®çš„ç©ºé—´ç»´åº¦ã€‚\n",
        "\n",
        "### Q4: å¦‚ä½•è¿è¡Œå®Œæ•´è®­ç»ƒï¼Ÿ\n",
        "**A:** æŸ¥çœ‹æ–‡æ¡£ï¼š\n",
        "```bash\n",
        "cat doc/02-training/train.md\n",
        "cat doc/04-qwen-adaptation/QWEN_MODEL_ADAPTATION.md\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. å‚è€ƒèµ„æº\n",
        "\n",
        "### ğŸ“š é¡¹ç›®æ–‡æ¡£\n",
        "- **ä¸»ç´¢å¼•**: `doc/README.md`\n",
        "- **Qwen é€‚é…æŒ‡å—**: `doc/04-qwen-adaptation/QWEN_MODEL_ADAPTATION.md`\n",
        "- **æ•…éšœæ’é™¤**: `doc/05-troubleshooting/README.md`\n",
        "- **æµ‹è¯•æ–‡æ¡£**: `tests/README.md`\n",
        "\n",
        "### ğŸ§ª è¿è¡Œæµ‹è¯•\n",
        "```bash\n",
        "cd tests\n",
        "python test_frozen_qwen.py       # å®Œæ•´æµ‹è¯•\n",
        "python diagnose_image_grid_thw.py  # è¯Šæ–­å·¥å…·\n",
        "```\n",
        "\n",
        "### ğŸ”— å¤–éƒ¨é“¾æ¥\n",
        "- [HuggingFace Model](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)\n",
        "- [F-LMM è®ºæ–‡](https://arxiv.org/abs/2406.05821)\n",
        "\n",
        "### ğŸ’» ç›¸å…³ä»£ç \n",
        "- æ¨¡å‹å®ç°: `flmm/models/frozen_qwen.py`\n",
        "- é…ç½®æ–‡ä»¶: `configs/qwen/frozen_qwen2_5_vl_3b_instruct_unet_sam_l_refcoco_png.py`\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ‰ æ€»ç»“\n",
        "\n",
        "ä½ å·²ç»å­¦ä¹ äº†ï¼š\n",
        "1. âœ… Qwen2.5-VL çš„æ ¸å¿ƒç‰¹ç‚¹å’Œä¸å…¶ä»–æ¨¡å‹çš„åŒºåˆ«\n",
        "2. âœ… æ¨¡å‹åŠ è½½ã€å›¾åƒå¤„ç†å’Œæ¨ç†\n",
        "3. âœ… æå–æ³¨æ„åŠ›å’Œéšè—å±‚ï¼ˆF-LMM éœ€è¦ï¼‰\n",
        "4. âœ… ç†è§£ image_grid_thw å’Œ pixel_values\n",
        "5. âœ… ä¸ F-LMM çš„é›†æˆæ–¹å¼\n",
        "\n",
        "### ä¸‹ä¸€æ­¥\n",
        "- è¿è¡Œå®Œæ•´è®­ç»ƒ: `doc/02-training/train.md`\n",
        "- æ·±å…¥ç†è§£æ¶æ„: `doc/01-architecture/gykreadme.md`\n",
        "- æŸ¥çœ‹æ›´å¤šç¤ºä¾‹: `scripts/demo/`\n",
        "\n",
        "**æœ€åæ›´æ–°**: 2025-11-09  \n",
        "**ä½œè€…**: AI Assistant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯¼å…¥å¿…è¦çš„åº“\n",
        "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "# åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨\n",
        "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
        "\n",
        "# å‡†å¤‡å›¾åƒå’Œæ–‡æœ¬\n",
        "image = Image.open(requests.get(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\", stream=True).raw)\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": image},\n",
        "            {\"type\": \"text\", \"text\": \"Describe the image.\"},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "\n",
        "# ä½¿ç”¨processorå¤„ç†è¾“å…¥\n",
        "inputs = processor.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_dict=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "inputs_tensor = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
        "\n",
        "print(\"å¤„ç†åçš„è¾“å…¥:\")\n",
        "for key, value in inputs_tensor.items():\n",
        "    if isinstance(value, torch.Tensor):\n",
        "        print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "# ç”Ÿæˆè¾“å‡º\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "print(generated_text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "flmm-qwen-py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
