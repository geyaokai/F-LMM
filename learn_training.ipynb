{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94219e4a",
   "metadata": {},
   "source": [
    "## 深入理解训练流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed273d23",
   "metadata": {},
   "source": [
    "## 1.1 数据加载机制（优先）\n",
    "**重点文件：**\n",
    "- flmm/datasets/png.py - PNG 数据集\n",
    "- flmm/datasets/transforms.py - 数据转换\n",
    "- configs/deepseek_vl/frozen_deepseek_vl_1_3b_chat_unet_sam_l_refcoco_png.py (数据集配置部分)\n",
    "学习要点：\n",
    "- 如何加载图像和标注\n",
    "- prompt 模板如何构建\n",
    "- 数据增强和预处理流程\n",
    "- batch 组织方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcccdeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a47a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.19 (main, Oct 21 2025, 16:43:05) [GCC 11.2.0]\n",
      "PyTorch: 2.2.2+cu118\n",
      "CUDA available: True\n",
      "GPU 数量: 1\n",
      "当前 GPU: 0\n",
      "GPU 名称: NVIDIA A800 80GB PCIe\n",
      "Transformers: 4.51.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 设置使用 3 号 GPU\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "os.environ[ 'HF_ENDPOINT'] =\"https://hf-mirror.com\"\n",
    "# # 你的代理地址\n",
    "# proxy_url = \"http://10.156.216.30:16371\" \n",
    "\n",
    "# # 设置 HTTP 和 HTTPS 代理\n",
    "# os.environ['http_proxy'] = proxy_url\n",
    "# os.environ['https_proxy'] = proxy_url\n",
    "# os.environ['HTTP_PROXY'] = proxy_url # 建议同时设置大小写\n",
    "# os.environ['HTTPS_PROXY'] = proxy_url\n",
    "# 检查环境\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU 数量: {torch.cuda.device_count()}\")\n",
    "        print(f\"当前 GPU: {torch.cuda.current_device()}\")\n",
    "        print(f\"GPU 名称: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed!\")\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"Transformers: {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Transformers not installed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa78fe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvprtemp/miniconda3/envs/flmm-qwen-py310/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/cvprtemp/miniconda3/envs/flmm-qwen-py310/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlvsym'\n",
      "/home/cvprtemp/miniconda3/envs/flmm-qwen-py310/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/home/cvprtemp/miniconda3/envs/flmm-qwen-py310/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/home/cvprtemp/miniconda3/envs/flmm-qwen-py310/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/home/cvprtemp/miniconda3/envs/flmm-qwen-py310/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=3.97s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping Qwen processor with QwenImageProcessorWrapper\n",
      "Image token ids: [151655], decoded: <|image_pad|>\n",
      "Wrapping Qwen processor with QwenImageProcessorWrapper\n",
      "Image token ids: [151655], decoded: <|image_pad|>\n",
      "Wrapping Qwen processor with QwenImageProcessorWrapper\n",
      "Image token ids: [151655], decoded: <|image_pad|>\n",
      "Wrapping Qwen processor with QwenImageProcessorWrapper\n",
      "Image token ids: [151655], decoded: <|image_pad|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from flmm.datasets.png import PNGDataset\n",
    "from mmengine.config import Config\n",
    "from xtuner.registry import BUILDER\n",
    "\n",
    "# 加载配置\n",
    "cfg = Config.fromfile('configs/qwen/frozen_qwen2_5_vl_7b_instruct_unet_sam_l_refcoco_png.py')\n",
    "# cfg = Config.fromfile('configs/deepseek_vl/frozen_deepseek_vl_1_3b_chat_unet_sam_l_refcoco_png.py')\n",
    "# 从配置构建数据集对象（不是配置字典）\n",
    "dataset = BUILDER.build(cfg.train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ef9ddc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集大小: 190157\n",
      "In QwenImageProcessorWrapper, result[\"image_grid_thw\"].shape is \n",
      " torch.Size([1, 3])\n",
      "In PNGDataset, image_grid_thw is \n",
      " [[1, 30, 46]]\n",
      "In PNGDataset, result_dict[\"image_grid_thw\"].shape is \n",
      " torch.Size([1, 3])\n",
      "dict_keys(['input_ids', 'mask_ids', 'pixel_values', 'padded_masks', 'masks', 'gt_masks', 'image_sizes', 'mask_infos', 'image', 'file_name', 'meta_data', 'labels', 'image_grid_thw'])\n",
      "\n",
      "第一个样本的键:\n",
      "  input_ids: Tensor shape torch.Size([390]), dtype torch.int64\n",
      "  mask_ids: Tensor shape torch.Size([390]), dtype torch.int64\n",
      "  pixel_values: Tensor shape torch.Size([1380, 1176]), dtype torch.float32\n",
      "  padded_masks: Tensor shape torch.Size([2, 420, 644]), dtype torch.uint8\n",
      "  masks: Tensor shape torch.Size([2, 420, 644]), dtype torch.uint8\n",
      "  gt_masks: Tensor shape torch.Size([2, 426, 640]), dtype torch.uint8\n",
      "  image_sizes: Tensor shape torch.Size([2]), dtype torch.int64\n",
      "  mask_infos: list of length 2\n",
      "  image: JpegImageFile = <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x426 at 0x7FE68D323CD0>\n",
      "  file_name: str = 000000226461.jpg\n",
      "  meta_data: dict = {'image_shape': {'height': 420, 'width': 644}, 'padded_shape': {'height': 420, 'width': 644}, 'padding': {'before_height': 0, 'after_height': 0, 'before_width': 0, 'after_width': 0}, 'scale_factor': (0.9859154929577465, 1.00625), 'original_shape': {'height': 426, 'width': 640}}\n",
      "  labels: Tensor shape torch.Size([390]), dtype torch.int64\n",
      "  image_grid_thw: Tensor shape torch.Size([1, 3]), dtype torch.int64\n"
     ]
    }
   ],
   "source": [
    "# 检查数据集\n",
    "print(f\"数据集大小: {len(dataset)}\")\n",
    "\n",
    "# 获取第一个样本\n",
    "if len(dataset) > 0:\n",
    "    data_sample = dataset[0]\n",
    "    print(data_sample.keys())\n",
    "    print(\"\\n第一个样本的键:\")\n",
    "    for key, value in data_sample.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"  {key}: Tensor shape {value.shape}, dtype {value.dtype}\")\n",
    "        elif isinstance(value, (list, tuple)):\n",
    "            print(f\"  {key}: {type(value).__name__} of length {len(value)}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {type(value).__name__} = {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca0423",
   "metadata": {},
   "source": [
    "## 数据流图\n",
    "0. 原始 PNG 数据<br>\n",
    "  ↓\n",
    "1. 加载图像和分割图<br>\n",
    "  ↓\n",
    "2. Tokenize prompt + captions → input_ids<br>\n",
    "  ↓\n",
    "3. 构建 mask_ids（标记每个 token 属于哪个 mask）<br>\n",
    "  ↓\n",
    "4. 图像预处理 → pixel_values (384×384)<br>\n",
    "  ↓\n",
    "5. 提取和调整 mask → masks (255×384)<br>\n",
    "  ↓\n",
    "6. Padding mask → padded_masks (384×384)<br>\n",
    "  ↓\n",
    "7. 保存原始尺寸 → gt_masks (426×640)<br>\n",
    "  ↓\n",
    "8. 构建 labels（用于语言建模）<br>\n",
    "  ↓<br>\n",
    "输出 data_sample 字典<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7044c7bd",
   "metadata": {},
   "source": [
    "## 2.2 模型结构理解\n",
    "\n",
    "**重点文件：**\n",
    "- `flmm/models/frozen_deepseek_vl.py` - DeepSeekVL 冻结模型\n",
    "- `flmm/models/mask_head/mask_decoder.py` - U-Net 分割头\n",
    "- `flmm/models/mask_head/mask_refiner.py` - SAM 包装器\n",
    "\n",
    "**学习要点：**\n",
    "- 如何冻结基础 LMM 参数\n",
    "- 如何提取 hidden states 和 attentions\n",
    "- 注意力如何映射到图像空间\n",
    "- U-Net 和 SAM 如何集成\n",
    "- 损失计算方式（Dice Loss + CrossEntropy Loss）\n",
    "\n",
    "**对比学习：**\n",
    "- 查看 `frozen_llava.py` 和 `frozen_mgm.py`，了解不同模型的适配方式\n",
    "\n",
    "**详细文档：** 参见 `doc/MODEL_STRUCTURE.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa706983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.2 理解注意力提取过程（不加载模型版本）\n",
    "import torch\n",
    "\n",
    "# 获取一个数据样本\n",
    "data_sample = dataset[0]\n",
    "\n",
    "# 从 tokenizer 获取图像 token 索引\n",
    "tokenizer = BUILDER.build(cfg.tokenizer)\n",
    "image_token_idx = tokenizer.encode('<image_placeholder>', add_special_tokens=False)[-1]\n",
    "\n",
    "# 从配置获取 clip_shape（DeepSeekVL 固定为 24）\n",
    "clip_shape = 24  # DeepSeekVL 使用 24×24 的特征图\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"注意力提取流程\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. 输入准备:\")\n",
    "print(f\"   input_ids shape: {data_sample['input_ids'].shape}\")\n",
    "print(f\"   pixel_values shape: {data_sample['pixel_values'].shape}\")\n",
    "print(f\"   mask_ids shape: {data_sample['mask_ids'].shape}\")\n",
    "\n",
    "print(\"\\n2. 图像 token 位置:\")\n",
    "images_seq_mask = data_sample['input_ids'] == image_token_idx\n",
    "num_image_tokens = images_seq_mask.sum().item()\n",
    "image_token_positions = torch.where(images_seq_mask)[0].tolist()\n",
    "print(f\"   图像 token 索引: {image_token_idx}\")\n",
    "print(f\"   图像 token 数量: {num_image_tokens}\")\n",
    "print(f\"   图像 token 位置: {image_token_positions[:10]}... (共 {len(image_token_positions)} 个)\")\n",
    "\n",
    "print(\"\\n3. 注意力重塑:\")\n",
    "print(f\"   clip_shape: {clip_shape}×{clip_shape} = {clip_shape*clip_shape} 个图像 patch\")\n",
    "print(f\"   注意力权重将重塑为: [num_heads, seq_len, {clip_shape}, {clip_shape}]\")\n",
    "print(f\"   每个图像 token 对应一个 {clip_shape}×{clip_shape} 的特征图位置\")\n",
    "\n",
    "print(\"\\n4. Mask 分组:\")\n",
    "masks = data_sample['masks']\n",
    "print(f\"   mask 数量: {len(masks)}\")\n",
    "for mask_id in range(len(masks)):\n",
    "    matched = data_sample['mask_ids'] == mask_id\n",
    "    matched_count = matched.sum().item()\n",
    "    matched_positions = torch.where(matched)[0].tolist()\n",
    "    print(f\"   Mask {mask_id}: {matched_count} 个 token\")\n",
    "    print(f\"      Token 位置: {matched_positions[:5]}...\" if matched_count > 5 else f\"      Token 位置: {matched_positions}\")\n",
    "\n",
    "print(\"\\n5. 注意力提取过程（概念性）:\")\n",
    "print(\"   a) 前向传播 LMM，获取所有层的 attention weights\")\n",
    "print(\"   b) 提取 attention[:, :, image_token_positions] → [num_heads, seq_len, num_image_tokens]\")\n",
    "print(\"   c) Reshape: [num_heads, seq_len, num_image_tokens] → [num_heads, seq_len, 24, 24]\")\n",
    "print(\"   d) 对于每个 mask，提取对应 token 的注意力\")\n",
    "print(\"   e) Merge heads: [num_heads, H, W] → [H, W] (mean/max)\")\n",
    "print(\"   f) Concat layers: [num_layers, H, W] → [num_layers*num_heads, H, W]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(masks.shape)\n",
    "print(masks)\n",
    "print(mask_id)\n",
    "print(data_sample['mask_ids'].shape)\n",
    "print(data_sample['mask_ids'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flmm-qwen-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
